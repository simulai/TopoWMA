{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ea1170",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-25T00:47:52.827577Z",
     "iopub.status.busy": "2025-02-25T00:47:52.827271Z",
     "iopub.status.idle": "2025-02-25T00:47:53.608639Z",
     "shell.execute_reply": "2025-02-25T00:47:53.607895Z"
    },
    "papermill": {
     "duration": 0.786006,
     "end_time": "2025-02-25T00:47:53.610259",
     "exception": false,
     "start_time": "2025-02-25T00:47:52.824253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a94416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T00:47:53.615173Z",
     "iopub.status.busy": "2025-02-25T00:47:53.614731Z",
     "iopub.status.idle": "2025-02-25T00:48:30.517271Z",
     "shell.execute_reply": "2025-02-25T00:48:30.516277Z"
    },
    "papermill": {
     "duration": 36.906775,
     "end_time": "2025-02-25T00:48:30.518945",
     "exception": false,
     "start_time": "2025-02-25T00:47:53.612170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Total Loss: 1.0624, Recon: 0.3391, KL: 0.0302, Adv: 0.6931\n",
      "Epoch [2/10] Total Loss: 1.0287, Recon: 0.3341, KL: 0.0012, Adv: 0.6933\n",
      "Epoch [3/10] Total Loss: 1.0275, Recon: 0.3337, KL: 0.0004, Adv: 0.6933\n",
      "Epoch [4/10] Total Loss: 1.0271, Recon: 0.3336, KL: 0.0003, Adv: 0.6932\n",
      "Epoch [5/10] Total Loss: 1.0271, Recon: 0.3336, KL: 0.0001, Adv: 0.6934\n",
      "Epoch [6/10] Total Loss: 1.0270, Recon: 0.3336, KL: 0.0001, Adv: 0.6933\n",
      "Epoch [7/10] Total Loss: 1.0270, Recon: 0.3336, KL: 0.0002, Adv: 0.6933\n",
      "Epoch [8/10] Total Loss: 1.0266, Recon: 0.3336, KL: 0.0002, Adv: 0.6929\n",
      "Epoch [9/10] Total Loss: 1.0269, Recon: 0.3336, KL: 0.0001, Adv: 0.6932\n",
      "Epoch [10/10] Total Loss: 1.0269, Recon: 0.3336, KL: 0.0001, Adv: 0.6932\n",
      "Training complete, model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# 定义 VAE 模型（编码器+解码器）\n",
    "# ------------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # 编码器部分：将输入映射到隐变量参数\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # 均值\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # 对数方差\n",
    "        \n",
    "        # 解码器部分：将隐变量恢复为输入\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Tanh()  # 假设输入数据归一化到 [-1, 1]\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar, z\n",
    "\n",
    "# ------------------------------\n",
    "# 定义隐变量判别器（Latent Discriminator）\n",
    "# ------------------------------\n",
    "class LatentDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=64):\n",
    "        super(LatentDiscriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "# ------------------------------\n",
    "# 定义各项损失函数\n",
    "# ------------------------------\n",
    "def reconstruction_loss(recon_x, x):\n",
    "    # 使用均方误差作为重构损失\n",
    "    return nn.MSELoss()(recon_x, x)\n",
    "\n",
    "def kl_divergence(mu, logvar):\n",
    "    # KL 散度公式：-0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / mu.size(0)\n",
    "\n",
    "def adversarial_loss(discriminator, z, target_label):\n",
    "    criterion = nn.BCELoss()\n",
    "    pred = discriminator(z)\n",
    "    target = torch.full(pred.size(), target_label, device=pred.device, dtype=pred.dtype)\n",
    "    return criterion(pred, target)\n",
    "\n",
    "# ------------------------------\n",
    "# 主训练流程\n",
    "# ------------------------------\n",
    "def main():\n",
    "    # 超参数设定\n",
    "    input_dim = 384   # 例如使用 Sentence-BERT 生成的嵌入维度\n",
    "    hidden_dim = 128\n",
    "    latent_dim = 32\n",
    "    batch_size = 64\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    beta_kl = 1.0     # KL 损失权重\n",
    "    gamma_adv = 1.0   # 对抗损失权重\n",
    "    \n",
    "    # 这里为示例，我们使用随机生成的数据模拟输入（实际请替换为你自己的数据）\n",
    "    n_samples = 30000\n",
    "    # 数据归一化到 [-1, 1]\n",
    "    data = np.random.uniform(-1, 1, size=(n_samples, input_dim)).astype(np.float32)\n",
    "    dataset = TensorDataset(torch.from_numpy(data))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    vae = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "    latent_disc = LatentDiscriminator(latent_dim).to(device)\n",
    "    \n",
    "    # 定义优化器\n",
    "    optimizer_vae = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    optimizer_disc = optim.Adam(latent_disc.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_recon_loss = 0.0\n",
    "        total_kl_loss = 0.0\n",
    "        total_adv_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            # ----- Step 1: 训练判别器 -----\n",
    "            vae.eval()           # 冻结 VAE\n",
    "            latent_disc.train()  # 判别器开启训练\n",
    "            optimizer_disc.zero_grad()\n",
    "            \n",
    "            # 真实隐变量样本（来自标准正态分布）\n",
    "            z_real = torch.randn(x.size(0), latent_dim, device=device)\n",
    "            # 生成的隐变量样本（来自 VAE 编码器）\n",
    "            with torch.no_grad():\n",
    "                _, _, _, z_fake = vae(x)\n",
    "            \n",
    "            loss_disc_real = adversarial_loss(latent_disc, z_real, 1)  # 真实标签为 1\n",
    "            loss_disc_fake = adversarial_loss(latent_disc, z_fake, 0)  # 虚假标签为 0\n",
    "            loss_disc = loss_disc_real + loss_disc_fake\n",
    "            loss_disc.backward()\n",
    "            optimizer_disc.step()\n",
    "            \n",
    "            # ----- Step 2: 训练 VAE（编码器+解码器） -----\n",
    "            vae.train()\n",
    "            latent_disc.eval()   # 冻结判别器\n",
    "            optimizer_vae.zero_grad()\n",
    "            \n",
    "            recon, mu, logvar, z = vae(x)\n",
    "            loss_recon = reconstruction_loss(recon, x)\n",
    "            loss_kl = kl_divergence(mu, logvar)\n",
    "            # 对抗性损失：希望判别器将编码器生成的 z 判定为“真实”（标签=1）\n",
    "            loss_adv = adversarial_loss(latent_disc, z, 1)\n",
    "            \n",
    "            loss = loss_recon + beta_kl * loss_kl + gamma_adv * loss_adv\n",
    "            loss.backward()\n",
    "            optimizer_vae.step()\n",
    "            \n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_recon_loss += loss_recon.item() * x.size(0)\n",
    "            total_kl_loss += loss_kl.item() * x.size(0)\n",
    "            total_adv_loss += loss_adv.item() * x.size(0)\n",
    "            \n",
    "        avg_loss = total_loss / n_samples\n",
    "        avg_recon_loss = total_recon_loss / n_samples\n",
    "        avg_kl_loss = total_kl_loss / n_samples\n",
    "        avg_adv_loss = total_adv_loss / n_samples\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Total Loss: {avg_loss:.4f}, Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f}, Adv: {avg_adv_loss:.4f}\")\n",
    "    \n",
    "    # 保存训练好的模型\n",
    "    torch.save(vae.state_dict(), \"vae_with_latent_disc.pth\")\n",
    "    print(\"Training complete, model saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.879742,
   "end_time": "2025-02-25T00:48:32.812758",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-25T00:47:49.933016",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
